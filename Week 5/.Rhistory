sum(wiki$Vandal == 1)
## Bags of Words
## How many terms appear in dtmAdded?
Sys.setlocale("LC_ALL", "C")
corpusAdded = VCorpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded, removeWords, stopwords("english"))
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
## Filter out sparse terms by keeping only terms that appear in 0.3% or more of the revisions, and call the new matrix sparseAdded.
## How many terms appear in sparseAdded?
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
str(wordsAdded)
Sys.setlocale("LC_ALL", "C")
corpusRemoved = VCorpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
nrow(wordsRemoved)
str(wordsRemoved)
sparseRemoved
ncol(wordsRemoved)
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
section = sample.split(wikiWords, SplitRatio = 0.7)
test = subset(wikiWords, section == FALSE)
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
set.seed(123)
section = sample.split(wikiWords, SplitRatio = 0.7)
train = subset(wikiWords, section == TRUE)
test = subset(wikiWords, section == FALSE)
table(test$Vandal)
(622)/(622+542)
wikiCART = rpart(Vandal ~., data = train)
wikiCART.pred = predict(wikiCART, newdata = test)
wikiCART.pred
a = table(test$Vandal, wikiCART.pred > 0.5)
a = table(test$Vandal, wikiCART.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
rpart.plot(wikiCART)
prp(wikiCART)
rpart.plo(wikiCART)
rpart.plot(wikiCART)
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
sum(wikiWords2$HTTP)
wikiTrain2 = subset(wikiWords2, section==TRUE)
wikiTest2 = subset(wikiWords2, section==FALSE)
wikiTrain2 = subset(wikiWords2, section==TRUE)
wikiTest2 = subset(wikiWords2, section==FALSE)
wikiCART2 = rpart(Vandal ~., data = wikiTrain2)
wikiCART2.pred = predict(wikiCART, newdata = wikiTest2)
a = table(test$Vandal, wikiCART2.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
wikiCART2.pred = predict(wikiCART2, newdata = wikiTest2)
a = table(test$Vandal, wikiCART2.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
a = table(wikiTest2$Vandal, wikiCART2.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
table(wikiTest2$Vandal, wikiCART2.pred[,2] > 0.5)
wikiTrain2 = subset(wikiWords2, section == TRUE)
wikiTest2 = subset(wikiWords2, section == FALSE)
wikiCART2 = rpart(Vandal ~., data = wikiTrain2)
wikiCART2.pred = predict(wikiCART2, newdata = wikiTest2)
a = table(wikiTest2$Vandal, wikiCART2.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
rpart.plot(wikiCART2)
wikiTrain2 = subset(wikiWords2, section == TRUE)
wikiTest2 = subset(wikiWords2, section == FALSE)
wikiCART2 = rpart(Vandal ~., data = wikiTrain2)
wikiCART2.pred = predict(wikiCART2, newdata = wikiTest2)
a = table(wikiTest2$Vandal, wikiCART2.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
library(tm)
library(SnowballC)
library(caTools)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ROCR)
## Load the data wiki.csv, calling the data frame "wiki". Convert the "Vandal" column to a factor
## How many cases of vandalism were detected in the history of this page?
wiki = read.csv("wiki.csv", stringsAsFactors=FALSE)
wiki$Vandal = as.factor(wiki$Vandal)
sum(wiki$Vandal == 1)
## Bags of Words
## How many terms appear in dtmAdded?
Sys.setlocale("LC_ALL", "C")
corpusAdded = VCorpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded, removeWords, stopwords("english"))
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
## Filter out sparse terms by keeping only terms that appear in 0.3% or more of the revisions, and call the new matrix sparseAdded.
## How many terms appear in sparseAdded?
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
## Convert sparseAdded to a data frame called wordsAdded, and then prepend all the words with the letter A
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
## Now repeat all of the steps we've done so far (create a corpus, remove stop words, stem the document, create a sparse document
## term matrix, and convert it to a data frame) to create a Removed bag-of-words dataframe, called wordsRemoved, except this time,
## prepend all of the words with the letter R. How many words are in the wordsRemoved data frame?
Sys.setlocale("LC_ALL", "C")
corpusRemoved = VCorpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
ncol(wordsRemoved)
## What is the accuracy on the test set of a baseline method that always predicts "not vandalism" (the most frequent outcome)?
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
set.seed(123)
section = sample.split(wikiWords, SplitRatio = 0.7)
train = subset(wikiWords, section == TRUE)
test = subset(wikiWords, section == FALSE)
table(test$Vandal)
(622)/(622+542)
## Build a CART model to predict Vandal, using all of the other variables as independent variables.
## What is the accuracy of the model on the test set, using a threshold of 0.5?
wikiCART = rpart(Vandal ~., data = train)
wikiCART.pred = predict(wikiCART, newdata = test)
a = table(test$Vandal, wikiCART.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
## Plot the CART tree. How many word stems does the CART model use?
rpart.plot(wikiCART)
## Based on this new column, how many revisions added a link?
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
sum(wikiWords2$HTTP)
## What is the new accuracy of the CART model on the test set, using a threshold of 0.5?
wikiTrain2 = subset(wikiWords2, section == TRUE)
wikiTest2 = subset(wikiWords2, section == FALSE)
wikiCART2 = rpart(Vandal ~., data = wikiTrain2)
wikiCART2.pred = predict(wikiCART2, newdata = wikiTest2)
a = table(wikiTest2$Vandal, wikiCART2.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
a = table(wikiTest2$Vandal, wikiCART2.pred[,2] >= 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
rpart.plot(wikiCART2)
wikiTrain2 = subset(wikiWords2, section == TRUE)
wikiTest2 = subset(wikiWords2, section == FALSE)
wikiCART2 = rpart(Vandal ~ HTTP., data = wikiTrain2)
wikiCART2.pred = predict(wikiCART2, newdata = wikiTest2)
rpart.plot(wikiCART2)
a = table(wikiTest2$Vandal, wikiCART2.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
wikiCART2 = rpart(Vandal ~ HTTP., data = wikiTrain2)
wikiCART2 = rpart(Vandal ~ HTTP, data = wikiTrain2)
wikiCART2.pred = predict(wikiCART2, newdata = wikiTest2)
rpart.plot(wikiCART2)
a = table(wikiTest2$Vandal, wikiCART2.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
wikiCART2 = rpart(Vandal ~ ., data = wikiTrain2)
wikiCART2.pred = predict(wikiCART2, newdata = wikiTest2)
rpart.plot(wikiCART2)
a = table(wikiTest2$Vandal, wikiCART2.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
mean(wikiWords2$NumWordsAdded)
wikiTrain3 = subset(wikiWords2, section == TRUE)
wikiTest3 = subset(wikiWords2, section == FALSE)
wikiCART3 = rpart(Vandal ~ ., data = wikiTrain3)
wikiCART3.pred = predict(wikiCART3, newdata = wikiTest3)
rpart.plot(wikiCART3)
a = table(wikiTest2$Vandal, wikiCART3.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
wikiTrain4 = subset(wikiWords3, section == TRUE)
wikiTest4 = subset(wikiWords3, section == FALSE)
wikiCART4 = rpart(Vandal ~ ., data = wikiTrain4)
wikiCART4.pred = predict(wikiCART3, newdata = wikiTest4)
rpart.plot(wikiCART4)
a = table(wikiTest2$Vandal, wikiCART4.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
wikiCART4 = rpart(Vandal ~ ., data = wikiTrain4)
wikiCART4.pred = predict(wikiCART4, newdata = wikiTest4)
rpart.plot(wikiCART4)
a = table(wikiTest2$Vandal, wikiCART4.pred[,2] > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
rpart.plot(wikiCART4)
prp(wikiCART4)
library(tm)
library(SnowballC)
library(caTools)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ROCR)
library(tm)
library(SnowballC)
library(caTools)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ROCR)
trials = read.csv("clinical_trial.csv", stringsAsFactors = FALSE)
str(trials)
nchar(trials$abstract)
max(nchar(trials$abstract))
sum(nchar(trials$abstract) == 0)
min(nchar(trials$title))
which.min(nchar(trials$title))
trials$title[which.min(nchar(trials$title)),]
trials$title[,which.min(nchar(trials$title))]
trials$title[which.min(nchar(trials$title))]
Sys.setlocale("LC_ALL", "C")
corpusTitle = VCorpus(VectorSource(trials$title))
corpusAbstract = VCorpus(VectorSource(trials$abstract))
Sys.setlocale("LC_ALL", "C")
corpusTitle = VCorpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, content_transformer(tolower))
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusTitle = tm_map(corpus, removeWords, stopwords("english"))
corpusTitle = tm_map(corpus, stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmTitle = removeSparseTerms(dtm, 0.95)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
Sys.setlocale("LC_ALL", "C")
corpusTitle = VCorpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, content_transformer(tolower))
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusTitle = tm_map(corpusTitle, stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmTitle = removeSparseTerms(dtm, 0.95)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
corpusTitle = VCorpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, content_transformer(tolower))
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusTitle = tm_map(corpusTitle, stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
corpusAbstract = VCorpus(VectorSource(trials$abstract))
corpusAbstract = tm_map(corpusAbstract, content_transformer(tolower))
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removeWords, stopwords("english"))
corpusAbstract = tm_map(corpusAbstract, stemDocument)
dtmAbstract= DocumentTermMatrix(corpusAbstract)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
dtmTitle
str(dtmTitle )
str(dtmAbstract)
colSums(dtmAbstract)
sort(colSums(dtmAbstract))
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
dtm = cbind(dtmTitle, dtmAbstract)
dtm$trial = trials$trial
ncol(dtm)
set.seed(144)
section = sample.split(dtm, SplitRatio = .7)
train = subset(dtm, section == TRUE)
test = subset(dtm, section == FALSE)
table(train$trial)
726/(726+574)
trialCART = rpart(trial ~., data = train)
trialCART = rpart(trial ~., data = train, method="class" )
rpart.plot(trialCART)
trialCART.pred = predict(trialCART)
max(trialCART.pred[,2])
max(trialCART.pred)
max(trialCART.pred[,2])
trialCART.pred[,2]
summary(trialCART.pred[,2])
max(trialCART.pred[,2])
max(trialCART.pred[,1])
max(trialCART.pred[,2])
max(trialCART.pred[,3])
max(trialCART.pred[,1])
max(trialCART.pred[,2])
summary(trialCART.pred[,2])
hist(trialCART.pred[,2])
summary(trialCART.pred[,2])
trialCART.pred = predict(trialCART)[,2]
summary(trialCART.pred)
table(train$trial, trialCART.pred > 0.5)
a = table(train$trial, trialCART.pred > 0.5)
(a[1,1]+a[2,2])/sum(a) #accuracy
table(train$trial, trialCART.pred > 0.5)
a = table(train$trial, trialCART.pred > 0.5)
TP = a[2,2] #true positives
TN = a[1,1] #true negatives
FP = a[2,1] #false positives
FN = a[1,2] #false negatives
sensitivity = TP/(TP+FN)
specificity = TN/(TN+FN)
accuracy = (TN + TP)/(TN + TP + FP + FN)
accuracy
sensitivity
specificity
rpart.plot(trialCART)
trialCART.pred = predict(trialCART)[,2]
summary(trialCART.pred)
library(tm)
library(SnowballC)
library(caTools)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ROCR)
## Load clinical_trial.csv into a data frame called trials
## How many characters are there in the longest abstract?
trials = read.csv("clinical_trial.csv", stringsAsFactors = FALSE)
max(nchar(trials$abstract))
## How many search results provided no abstract?
sum(nchar(trials$abstract) == 0)
## Find the observation with the minimum number of characters in the title
## What is the text of the title of this article?
trials$title[which.min(nchar(trials$title))]
## Preparing the Corpus
Sys.setlocale("LC_ALL", "C")
corpusTitle = VCorpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, content_transformer(tolower))
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusTitle = tm_map(corpusTitle, stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
corpusAbstract = VCorpus(VectorSource(trials$abstract))
corpusAbstract = tm_map(corpusAbstract, content_transformer(tolower))
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removeWords, stopwords("english"))
corpusAbstract = tm_map(corpusAbstract, stemDocument)
dtmAbstract= DocumentTermMatrix(corpusAbstract)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
## What is the most frequent word stem across all the abstracts?
sort(colSums(dtmAbstract))
##
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
## How many columns are in this combined data frame?
dtm = cbind(dtmTitle, dtmAbstract)
dtm$trial = trials$trial
ncol(dtm)
## What is the accuracy of the baseline model on the training set?
set.seed(144)
section = sample.split(dtm$trial, SplitRatio = .7)
train = subset(dtm, section == TRUE)
test = subset(dtm, section == FALSE)
table(train$trial)
726/(726+574)
## What is the name of the first variable the model split on?
trialCART = rpart(trial ~., data = train, method="class")
rpart.plot(trialCART)
## What is the maximum predicted probability for any result?
trialCART.pred = predict(trialCART)[,2]
summary(trialCART.pred)
## What is the training set accuracy of the CART model?
a = table(train$trial, trialCART.pred > 0.5)
TP = a[2,2] #true positives
TN = a[1,1] #true negatives
FP = a[2,1] #false positives
FN = a[1,2] #false negatives
sensitivity = TP/(TP+FN)
specificity = TN/(TN+FN)
accuracy = (TN + TP)/(TN + TP + FP + FN)
accuracy
sensitivity
specificity
FP = a[1,2] #false positives
FN = a[1,2] #false negatives
sensitivity = TP/(TP+FN)
specificity = TN/(TN+FN)
accuracy = (TN + TP)/(TN + TP + FP + FN)
specificity
FN = a[2,1] #false negatives
sensitivity = TP/(TP+FN)
specificity = TN/(TN+FN)
accuracy = (TN + TP)/(TN + TP + FP + FN)
specificity
sensitivity
library(tm)
library(SnowballC)
library(caTools)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ROCR)
## Load clinical_trial.csv into a data frame called trials
## How many characters are there in the longest abstract?
trials = read.csv("clinical_trial.csv", stringsAsFactors = FALSE)
max(nchar(trials$abstract))
## How many search results provided no abstract?
sum(nchar(trials$abstract) == 0)
## Find the observation with the minimum number of characters in the title
## What is the text of the title of this article?
trials$title[which.min(nchar(trials$title))]
## Preparing the Corpus
Sys.setlocale("LC_ALL", "C")
corpusTitle = VCorpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, content_transformer(tolower))
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusTitle = tm_map(corpusTitle, stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
corpusAbstract = VCorpus(VectorSource(trials$abstract))
corpusAbstract = tm_map(corpusAbstract, content_transformer(tolower))
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removeWords, stopwords("english"))
corpusAbstract = tm_map(corpusAbstract, stemDocument)
dtmAbstract= DocumentTermMatrix(corpusAbstract)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
## What is the most frequent word stem across all the abstracts?
sort(colSums(dtmAbstract))
##
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
## How many columns are in this combined data frame?
dtm = cbind(dtmTitle, dtmAbstract)
dtm$trial = trials$trial
ncol(dtm)
## What is the accuracy of the baseline model on the training set?
set.seed(144)
section = sample.split(dtm$trial, SplitRatio = .7)
train = subset(dtm, section == TRUE)
test = subset(dtm, section == FALSE)
table(train$trial)
726/(726+574)
## What is the name of the first variable the model split on?
trialCART = rpart(trial ~., data = train, method="class")
rpart.plot(trialCART)
## What is the maximum predicted probability for any result?
trialCART.pred = predict(trialCART)[,2]
summary(trialCART.pred)
## What is the training set accuracy of the CART model?
a = table(train$trial, trialCART.pred > 0.5)
TP = a[2,2] #true positives
TN = a[1,1] #true negatives
FP = a[2,1] #false positives
FN = a[1,2] #false negatives
sensitivity = TP/(TP+FN)
specificity = TN/(TN+FN)
accuracy = (TN + TP)/(TN + TP + FP + FN)
a = table(train$trial, trialCART.pred > 0.5)
TP = a[2,2] #true positives
TN = a[1,1] #true negatives
FP = a[1,2] #false positives
FN = a[2,1] #false negatives
sensitivity = TP/(TP+FN)
specificity = TN/(TN+FN)
accuracy = (TN + TP)/(TN + TP + FP + FN)
FN = a[1,2] #false negatives
sensitivity = TP/(TP+FN)
specificity = TN/(TN+FN)
accuracy = (TN + TP)/(TN + TP + FP + FN)
a = table(train$trial, trialCART.pred >= 0.5)
TP = a[2,2] #true positives
TN = a[1,1] #true negatives
FP = a[1,2] #false positives
FN = a[1,2] #false negatives
sensitivity = TP/(TP+FN)
specificity = TN/(TN+FN)
accuracy = (TN + TP)/(TN + TP + FP + FN)
TP = a[2,2] #true positives
TN = a[1,1] #true negatives
FP = a[1,2] #false positives
FN = a[2,1] #false negatives
sensitivity = TP/(TP+FN)
specificity = TN/(TN+FN)
accuracy = (TN + TP)/(TN + TP + FP + FN)
table(train$trial, trialCART.pred >= 0.5)
FN
TN
FN
sensitivity = TP/(TP+FN)
specificity = TN/(TN+FP)
accuracy = (TN + TP)/(TN + TP + FP + FN)
predTest = predict(trialCART, newdata = test)[,2]
a = table(train$trial, predTest >= 0.5)
TP = a[2,2] #true positives
TN = a[1,1] #true negatives
FP = a[1,2] #false positives
FN = a[2,1] #false negatives
sensitivity = TP/(TP+FN)
specificity = TN/(TN+FP)
accuracy = (TN + TP)/(TN + TP + FP + FN)
predTest = predict(trialCART, newdata = test)[,2]
a = table(train$trial, predTest >= 0.5)
a = table(test$trial, predTest >= 0.5)
TP = a[2,2] #true positives
TN = a[1,1] #true negatives
FP = a[1,2] #false positives
FN = a[2,1] #false negatives
sensitivity = TP/(TP+FN)
specificity = TN/(TN+FP)
accuracy = (TN + TP)/(TN + TP + FP + FN)
accuracy
sum(spamLogPrediction < 0.00001)
ROCRpred = prediction(predTest, test$trial)
ROCRpred = prediction(predTest, test$trial)
ROCRpred.perf = performance(ROCRpred, "tpr", "fpr")
plot(ROCRpred.perf)
as.numeric(performance(ROCRpred, "auc")@y.values)
